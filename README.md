Evolution
=========

A simple to use optimization library based on evolutionary algorithms.

It can be used in both dart and flutter projects.

# Getting started

Add the dependency to your pubspec.yaml file:

```yaml
dependencies:
  evolution: #latest version
```

Add this import statement to your source files:

```dart
import 'package:evolutionSimple/evolution.dart';
```

To give it a try, simply run the example. 

```dart
dart ./example/main.dart 
```
The Result of the example solves the following optimization tasks: 

* Sphere100 is version of the sphere function with a global minimum at (100.0, ..., 100.0).
* Sphere100 (restricted) is the same problem to be solved within a restricted search space.
* Ackley10 is a version of the ackley function with a global minimum at (10.0, ..., 10.0).
* Ackley100 is  a version of the ackley function with a global minimum at (100.0, ..., 100.0).
* Ackley100 (restricted) is the same problem to be solved within a restricted search space.

# A simple algorithm

1. Generate an initial population of candidate solutions (Agents), each of which will have property values of 0.0: 

```dart
Population start = generatePopulation(
    sizeN, // number of [Agent]s in the population
    positions, // number of variables, i.e dimensionality of the problem
    Random(seed), // pass a random number generator object to the Population
    fitness, // gibe each Agents an evaluation function
  );
```

2. Mutate the population. You can specify a weight factor to control the impact of mutation. 

```dart
Population mutated = start.mutation();
```

3. Generate a differential population of size [diffN].  

```dart
Population differential = mutated.differential(diffN);
```

4. Select a portion of the population of [sizeN] as survivors.  

```dart
Population selected = differential.copy().sorted().select(sizeN);
```

5. Loop!


# Differential Evolution

Differential Evolution is defined as:

```dart

/// A version of Differential Evolution with unrestricted search space.
Agent diff(
  int positions, // number of variables, i.e dimensionality of the problem
  int sizeN, // number of [Agent]s in the population
  int bestN, // number of [Agent]s selected by fitness
  int randN, // number of [Agent]s randomly selected
  int diffN, // number of [Agent]s generated by differential evolution
  int seed, // seeding the random number generator
  int steps, // number of generations
  double w, // weighting factor used in differential evolution
  double Function(List<double>) fitness, // evaluation function
) {
  //int seed = DateTime.now().millisecond;
  Random r = Random(seed);

  int z = 0;
  Population p0 = generatePopulation(
    sizeN,
    positions,
    r,
    fitness,
  );

  while (z < steps) {
    double wz = w / ((z == 0 ? 1 : z)).toDouble();
    
    
    // Optional:
    // Selecting survivors randomly gives less fit 
    // Agents a chance to improve the result in case
    // search is trapped in a local minimum. 
    /*
    Population p01 = p0.copy();
    p01.shuffle(r.r);
    Population rand = p01.select(randN);
    */

    // best survivors
    Population p02 = p0.copy();
    Population p021 = p02.sorted();
    Population best = p021.select(bestN);

    // mutation
    Population p03 = p0.copy();
    Population mutated = p03.mutation(wz / 10.0);

    Population differential = mutated.differential(diffN, wz * 10.0);

    // combine candidate solutions
    Population all = Population(
        /*rand + */ best + differential,
        r,
        fitness);
    Population p8 = all.copy();
    Population p9 = p8.sorted();
    Population p10 = p9.select(sizeN);

    p0 = p10;
    z++;
  }
  Population res = p0.sorted().select(1);
  return res.first;
}
```

To improve performace, use a restricted search space and the corresponding version of the algorithm: 

```dart
/// Version of [diff] for opitimizing with an restricted search space.
Agent diff2(
  int positions, //dimensionality of the problem
  int sizeN, // number of solution candidates to be entered in each new generation 
  int bestN, // number of best solutions selected within on generation 
  int randN, // number of random solutions selected within on generation with arbitrary fitness
  int diffN, //  number of solution candidates generated by differential process in  each generation 
  int seed, // initializing the pseudo-random number generator 
  int steps, // number of generations 
  double w, // spread of mutation 
  double Function(List<double>) fitness, // evaluation function for solution candidates
  double lower, // lower bound of the search space
  double upper, // upper bound of the search space
) {
  //int seed = DateTime.now().millisecond;
  Random r = Random(seed);

  int z = 0;
  Population p0 = generatePopulation(sizeN, positions, r, fitness);

  while (z < steps) {
    double wz = w / ((z == 0 ? 1 : z)).toDouble();

    // selecting survivors randomly gives less fit 
    // solution a chance to improve the result in case
    // the search is trapped in a local minimum. 

    /*
    Population p01 = p0.copy();
    p01.shuffle(r.r);
    Population rand = p01.select(randN);
    */

    // best survivors
    Population p02 = p0.copy();
    Population p021 = p02.sorted();
    Population best = p021.select(bestN);

    // mutation
    Population p03 = p0.copy();
    Population mutated = p03.mutation(wz / 10.0).confined(lower, upper);

    Population differential = mutated.differential(diffN, wz * 10.0);

    // combine
    Population all = Population(
        /*rand + */ best + differential,
        r,
        fitness);
    Population p8 = all.copy();
    Population p9 = p8.sorted();
    Population p10 = p9.select(sizeN);

    p0 = p10;
    z++;
  }
  Population res = p0.sorted().select(1);
  return res.first;
}
```


# Evolutionary Algorithms

Evolutionary algorithms [EA] optimize a problem iteratively by trying to improve a candidate solution stochastically with regard to a given measure of quality.
An EA maintains a population of candidate solutions and creates new candidate solutions by mutation and recombination from existing ones keeping whichever candidate solution has the best score or fitness on the optimization problem. 
EAs make few assumptions about the problem being optimized except for restricting the search space of candidate solutions. In this way the optimization problem is treated as a black box that merely provides a measure of quality given a candidate solution in terms of its fitness and the gradient of the problem function is therefore not needed. 

EAs can be used for multidimensional real-valued functions but does not use the gradient of the problem being optimized, which means they do not require the optimization problem to be differentiable, as is required by classic optimization methods such as gradient descent and quasi-newton methods. They can therefore also be used on optimization problems that are not even continuous, are noisy, change over time, etc. EAs do not guarantee an optimal solution is ever found.

